{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDn_lVxg3Z2G"
      },
      "source": [
        "# **Langauage Transcritption**\n",
        "\n",
        "Managing voice records directly from google colab `using google transcription api` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI0pone2c7zM",
        "outputId": "96b77d56-96a5-437a-f7f6-48827dd76a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1).\n",
            "libportaudiocpp0 is already the newest version (19.6.0-1).\n",
            "portaudio19-dev is already the newest version (19.6.0-1).\n",
            "libasound2-dev is already the newest version (1.1.3-5ubuntu0.6).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: PyAudio in /usr/local/lib/python3.7/dist-packages (0.2.11)\n"
          ]
        }
      ],
      "source": [
        "##Installation of pyaudio library on Linux system\n",
        "##Note: Another format is used when laoding such library on a windows system\n",
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n",
        "!pip install PyAudio\n",
        "import pyaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMKx2shwiNqx",
        "outputId": "113d181d-81ca-456a-ba04-0726c9366ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.7/dist-packages (2.0.10)\n"
          ]
        }
      ],
      "source": [
        "##Necessary importation for code running\n",
        "!pip install webrtcvad "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WSGywoRoqOo",
        "outputId": "c6f23e90-71d9-433f-ab34-fb6195e568c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.7/dist-packages (3.8.1)\n"
          ]
        }
      ],
      "source": [
        "#Necessary importation for code running\n",
        "!pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aElTVnWmxgDz",
        "outputId": "82f7871f-8543-4cbe-df01-378489b89baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n"
          ]
        }
      ],
      "source": [
        "#Necessary importation for code running\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Necessary importation for code running\n",
        "!pip install fleep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u26L4_jIFtN"
      },
      "source": [
        "# **To DO (2)**\n",
        "\n",
        "1. Complete the implementation of this section such that it can work locally omnon a computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "ZcD34ym1di29",
        "outputId": "b4631972-1270-4df3-988e-9dbfdcde45fb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport wave\\nimport os, sys\\nimport librosa\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport librosa.display\\nimport tensorflow as tf\\nimport webrtcvad\\n\\nvad = webrtcvad.Vad()\\nvad.set_mode(2)\\nFORMAT = pyaudio.paInt16\\nCHANNELS = 1\\nRATE = 32000\\nCHUNK = 960\\nRECORD_SECONDS = 3\\nWAVE_OUTPUT_FILENAME = \"file.wav\"\\n\\naudio = pyaudio.PyAudio()\\n\\n# start Recording\\nstream = audio.open(format=FORMAT, channels=CHANNELS,\\n                rate=RATE, input=True,\\n                frames_per_buffer=CHUNK)\\n'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##Codes to be used in the when the jupyter file will be converted into pythin file\n",
        "\"\"\"\n",
        "import wave\n",
        "import os, sys\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "import webrtcvad\n",
        "\n",
        "vad = webrtcvad.Vad()\n",
        "vad.set_mode(2)\n",
        "FORMAT = pyaudio.paInt16\n",
        "CHANNELS = 1\n",
        "RATE = 32000\n",
        "CHUNK = 960\n",
        "RECORD_SECONDS = 3\n",
        "WAVE_OUTPUT_FILENAME = \"file.wav\"\n",
        "\n",
        "audio = pyaudio.PyAudio()\n",
        "\n",
        "# start Recording\n",
        "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
        "                rate=RATE, input=True,\n",
        "                frames_per_buffer=CHUNK)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vACK5ovW6t8n"
      },
      "source": [
        "Managing audio records on google colab. This will change while working locally. \n",
        "A developer will have to add a section for such implementation locally for test to be done, for those who will use the codes locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8FKOyAeqvt5"
      },
      "outputs": [],
      "source": [
        "# all imports\n",
        "from io import BytesIO\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "from IPython.display import Javascript\n",
        "from pydub import AudioSegment\n",
        "\n",
        "wavfile = 'record.wav'\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=3):\n",
        "  print(\"Speak Now...\")\n",
        "  display(Javascript(RECORD))\n",
        "  sec += 1\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  print(\"Done Recording !\")\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  return b #byte stream\n",
        "\n",
        "def verifyVoice(wavfile):\n",
        "  \"\"\"\n",
        "  Function to verify the wave file record type\n",
        "  Note: Not sure to be kept, seems not really necessary\n",
        "\n",
        "  Input: None\n",
        "  Output: Voice format\n",
        "  \"\"\"\n",
        "  import fleep\n",
        "  with open(wavfile, \"rb\") as file:\n",
        "      info = fleep.get(file.read(128))\n",
        "\n",
        "  print(info.extension)\n",
        "  \n",
        "  return info.extension[0]\n",
        "\n",
        "def displayvoice(audio):\n",
        "  import IPython.display as ipd\n",
        "  ipd.display(ipd.Audio(audio))\n",
        "\n",
        "def launchClientTranscript():\n",
        "  \"\"\"\n",
        "  Function to convert voice directly.\n",
        "  Note: This has been tested and works for the moment with english voice\n",
        "\n",
        "  Input: None\n",
        "  Output: voice format type, transcripted voice text\n",
        "  \"\"\"\n",
        "  import speech_recognition as speech_recog\n",
        "  rec = speech_recog.Recognizer() ##Google speech recognition API\n",
        "\n",
        "  with open(wavfile, 'wb') as file:\n",
        "       file.write(record()) ##saving the recorded file from audio\n",
        "  flac_audio = AudioSegment.from_file(wavfile)\n",
        "  flac_audio.export(\"record.wav\", format=\"wav\") ##Giving the recorded file an appropriate format wav format\n",
        "  with speech_recog.AudioFile(wavfile) as source:\n",
        "        audio_listened = rec.listen(source) ##Launching the API\n",
        "  try:\n",
        "    print(\"I think you said: \\n\" + rec.recognize_google(audio_listened, language='fr-FR')) ## the language='fr-FR' helps understand french voices (in addition to the english voice already present) during transcription\n",
        "    transcript = rec.recognize_google(audio_listened)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "  \n",
        "  return verifyVoice(wavfile), transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "3UBCvvfqEbQN",
        "outputId": "0f1568db-2611-491d-a41b-886d903db362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speak Now...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\nconst sleep  = time => new Promise(resolve => setTimeout(resolve, time))\nconst b2text = blob => new Promise(resolve => {\n  const reader = new FileReader()\n  reader.onloadend = e => resolve(e.srcElement.result)\n  reader.readAsDataURL(blob)\n})\nvar record = time => new Promise(async resolve => {\n  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n  recorder = new MediaRecorder(stream)\n  chunks = []\n  recorder.ondataavailable = e => chunks.push(e.data)\n  recorder.start()\n  await sleep(time)\n  recorder.onstop = async ()=>{\n    blob = new Blob(chunks)\n    text = await b2text(blob)\n    resolve(text)\n  }\n  recorder.stop()\n})\n",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Recording !\n",
            "I think you said: \n",
            "hello\n",
            "['wav']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('wav', 'hello')"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "launchClientTranscript()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2pjsAf0GQBA"
      },
      "source": [
        "# **TO DO (1)**\n",
        "\n",
        "1. Test this google api transcription for voices in French if everything goes oni goes on proper.\n",
        "\n",
        "2. Implement the google translate to convert French records into English transcripted from voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdID6FPsGv9u"
      },
      "outputs": [],
      "source": [
        "##Implement code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GoogleAPI_Transcription",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
